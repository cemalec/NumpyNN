layers: 
  - name: input
    type: Dense
    input_size: 784
    output_size: 64
    activation_function: 'ReLU'
  - name: hidden
    type: Dense
    input_size: 64
    output_size: 32
    activation_function: 'ReLU'
  - name: output
    type: Dense
    input_size: 784
    output_size: 64
    activation_function: 'SoftMax'
loss: CrossEntropyLoss
optimizer: 
  type: Adam
  learning_rate: 0.001
  beta1: 0.9
  beta2: 0.999
  epsilon: 1e-8